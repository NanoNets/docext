model_name: "Qwen/Qwen2.5-VL-7B-Instruct-AWQ" # Can be any huggingface model


## Gradio config
gradio_config:
  port: 7860
  share: true

## vLLM config
vllm_config:
  log_level: "debug"
  host: "0.0.0.0"
  port: 8000
  max_model_len: 15000 # decrease this if you run into memory issues
  gpu_memory_utilization: 0.98
  quantization: "awq"
