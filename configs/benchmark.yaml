tasks:
  - KIE # Key Information Extraction
  - OCR # Optical Character Recognition
  - VQA # Visual Question Answering

datasets: # leave it empty to consider all the datasets
models: [
  "gpt-4o-mini",
  "gpt-4.1-2025-04-14",
  "gpt-4o-2024-08-06",
  "gemini/gemini-2.0-flash",
  "gemini/gemini-2.5-flash-preview-04-17",
  "o4-mini-2025-04-16",
  # "openrouter/meta-llama/llama-4-maverick:free"
  ]


cache_dir: "./docext_benchmark_cache"
max_samples_per_dataset: 20 # set this to a positive number to limit the number of samples per dataset


# dataset configs
## KIE datasets
docile:
  annot_path: "/home/paperspace/projects/datasets/docile/data/docile/val.json"
  annotations_root: /home/paperspace/projects/datasets/docile/data/docile/annotations
  pdf_root: /home/paperspace/projects/datasets/docile/data/docile/pdfs
handwritten_forms:
  hf_name: "Rasi1610/DeathSe43_44_checkbox"
  test_split: "val"

## OCR datasets
ocr_handwriting:
  hf_name: "DataStudio/OCR_handwritting_HAT2023"
  test_split: "train" # Test set does not have GT annotations, we sample 1000 random samples from train set
  max_samples: 1000 # we take the min of max_samples_per_dataset and max_samples, if both are set

digital_ocr_diacritics:
  hf_name: "ademax/ocr_scan_vi_01"
  test_split: "test"
  max_samples: 1000 # we take the min of max_samples_per_dataset and max_samples, if both are set

## VQA datasets
chartqa:
  hf_name: "HuggingFaceM4/ChartQA"
  test_split: "test"
  max_samples: 1000 # we take the min of max_samples_per_dataset and max_samples, if both are set

docvqa:
  hf_name: "lmms-lab/DocVQA"
  test_split: "DocVQA"
  max_samples: 1000 # we take the min of max_samples_per_dataset and max_samples, if both are set


# Default Templates
KIE_default_template:
  system_prompt: "You are a helpful assistant that extracts information from a document."
  document_page_seperator: "Page {page_number}"
  user_prompt: "Extract the following {fields} from the above document. If a field is not present, return ''. Return the output in a valid JSON format as {output_format}."

OCR_default_template:
  system_prompt: 'You are an OCR model. Your task is to extract all visible and legible text from images with high accuracy and fidelity.\n\nOutput only the text that appears in the image.\n\nPreserve the natural reading order (e.g., top-to-bottom, left-to-right) as much as possible.\n\nDo not infer, summarize, or add information not explicitly visible in the image.\n\nIf the text is partially visible, extract as much as can be confidently read.\n\nMaintain original formatting when reasonable (e.g., line breaks, spacing).\n\nIf there is no readable text, respond with: ""\n\nYour primary goal is to be accurate, consistent, and faithful to the image content.'
  user_prompt: "Extract the text from the above document. Do not give any explanation. Just return the text."


VQA_default_template:
  system_prompt: "You are a helpful and intelligent Vision-Language Model (VLM). When a user shares an image, your primary task is to extract relevant visual and textual information from the image and answer the user's questions accurately based on the image content. If the user asks a question that cannot be answered from the image alone, return ''. Always ground your answers in the image content unless otherwise specified."
  document_page_seperator: "Page {page_number}"
  user_prompt: "Answer the following question based on the images shared: {question}. Do not give any explanation. Just return the answer. If the answer is a number, return it as a number and not a string (eg. Return 3 instead of Three)."

# model configs
gpt-4o-mini:
  max_tokens: 3000
  temperature: 0.0
  template:
    KIE: null
    OCR: null
    VQA: null

gpt-4.1-2025-04-14:
  max_tokens: 3000
  temperature: 0.0
  template:
    KIE: null

gpt-4o-2024-08-06:
  max_tokens: 3000
  temperature: 0.0
  template:
    KIE: null

o4-mini-2025-04-16:
  max_completion_tokens: 3000
  temperature: 1.0
  template:
    KIE: null

gemini/gemini-2.0-flash:
  max_tokens: 3000
  temperature: 0.0
  template:
    KIE: null

gemini/gemini-2.5-pro-exp-03-25:
  max_tokens: 3000
  temperature: 0.0

gemini/gemini-2.5-flash-preview-04-17:
  max_tokens: 3000
  temperature: 0.0

openrouter/meta-llama/llama-4-maverick:free:
  max_tokens: 3000
  temperature: 0.0
